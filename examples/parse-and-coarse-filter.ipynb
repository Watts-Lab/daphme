{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-18T15:49:33.021834Z",
     "iopub.status.busy": "2024-01-18T15:49:33.021530Z",
     "iopub.status.idle": "2024-01-18T15:49:33.036897Z",
     "shell.execute_reply": "2024-01-18T15:49:33.036232Z",
     "shell.execute_reply.started": "2024-01-18T15:49:33.021808Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "Current session configs: <tt>{'conf': {'spark.pyspark.python': '/home/hadoop/epidemics-venv/bin/python3', 'spark.pyspark.virtualenv.bin.path': '/home/hadoop/epidemics-venv/bin', 'spark.dynamicAllocation.enabled': 'true', 'spark.executor.memory': '16g', 'spark.driver.memory': '16g', 'spark.executor.cores': '6', 'spark.driver.cores': '6', 'spark.driver.maxResultSize': '20g', 'livy.server.session.timeout-check': 'false'}, 'proxyUser': 'user_thomas-li', 'kind': 'pyspark'}</tt><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "No active sessions."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%configure -f\n",
    "\n",
    "{\"conf\":\n",
    "     {\"spark.pyspark.python\":\"/home/hadoop/epidemics-venv/bin/python3\",\n",
    "      \"spark.pyspark.virtualenv.bin.path\":\"/home/hadoop/epidemics-venv/bin\",\n",
    "      \"spark.dynamicAllocation.enabled\":\"true\",\n",
    "      \"spark.executor.memory\":\"16g\",\n",
    "      \"spark.driver.memory\":\"16g\",\n",
    "      \"spark.executor.cores\":\"6\",\n",
    "      \"spark.driver.cores\":\"6\",\n",
    "      \"spark.driver.maxResultSize\":\"20g\",\n",
    "      \"livy.server.session.timeout-check\" :\"false\"}\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-18T15:48:25.935169Z",
     "iopub.status.busy": "2024-01-18T15:48:25.934936Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "97b279519feb462d8b423076a62f09d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tbody><tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>User</th><th>Current session?</th></tr><tr><td>0</td><td>application_1705592640666_0001</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-10-0-1-196.us-east-2.compute.internal:20888/proxy/application_1705592640666_0001/\" class=\"emr-proxy-link j-19BT7SYB98JE0 application_1705592640666_0001\" emr-resource=\"j-19BT7SYB98JE0\n",
       "\" application-id=\"application_1705592640666_0001\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-10-0-1-211.us-east-2.compute.internal:8042/node/containerlogs/container_1705592640666_0001_01_000001/livy\">Link</a></td><td>None</td><td>✔</td></tr></tbody></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "474627f0e13d426ba406cff84a05d0bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# as per chatgpt's recommendations (to try after break)\n",
    "\n",
    "%%configure -f\n",
    "\n",
    "{\"conf\":\n",
    "     {\n",
    "      \"spark.pyspark.python\":\"/home/hadoop/epidemics-venv/bin/python3\",\n",
    "      \"spark.pyspark.virtualenv.bin.path\":\"/home/hadoop/epidemics-venv/bin\",\n",
    "      \"spark.dynamicAllocation.enabled\":\"true\",\n",
    "      \"spark.dynamicAllocation.minExecutors\":\"44\",\n",
    "      \"spark.dynamicAllocation.maxExecutors\":\"88\",\n",
    "      \"spark.executor.instances\":\"44\",\n",
    "      \"spark.executor.memory\":\"21g\",\n",
    "      \"spark.driver.memory\":\"28g\",\n",
    "      \"spark.executor.cores\":\"5\",\n",
    "      \"spark.driver.cores\":\"5\",\n",
    "      \"spark.driver.maxResultSize\":\"20g\",\n",
    "      \"spark.serializer\":\"org.apache.spark.serializer.KryoSerializer\",\n",
    "      \"spark.kryoserializer.buffer.max\":\"1024m\",\n",
    "      \"spark.default.parallelism\":\"528\",\n",
    "      \"spark.sql.shuffle.partitions\":\"528\",\n",
    "      \"spark.memory.fraction\":\"0.8\",\n",
    "      \"spark.memory.storageFraction\":\"0.5\",\n",
    "      \"spark.shuffle.service.enabled\":\"true\",\n",
    "      \"spark.shuffle.compress\":\"true\",\n",
    "      \"spark.io.compression.codec\":\"snappy\",\n",
    "      \"spark.shuffle.file.buffer\":\"1m\",\n",
    "      \"spark.rdd.compress\":\"true\",\n",
    "      \"spark.executor.extraJavaOptions\":\"-XX:+UseG1GC\",\n",
    "      \"spark.driver.extraJavaOptions\":\"-XX:+UseG1GC\",\n",
    "      \"livy.server.session.timeout-check\":\"false\"\n",
    "     }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-18T16:14:25.799429Z",
     "iopub.status.busy": "2024-01-18T16:14:25.799206Z",
     "iopub.status.idle": "2024-01-18T16:14:25.848747Z",
     "shell.execute_reply": "2024-01-18T16:14:25.848186Z",
     "shell.execute_reply.started": "2024-01-18T16:14:25.799405Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a8516f3b8da4d86908183026d36ecbb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import json\n",
    "import shapely\n",
    "from shapely.geometry import Point, MultiPoint, Polygon, shape, box\n",
    "from shapely.ops import unary_union, transform\n",
    "\n",
    "from pyspark.sql import DataFrame, SparkSession\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, LongType, DoubleType\n",
    "\n",
    "import daphme as dm\n",
    "from daphme import cleaning\n",
    "\n",
    "from sedona.spark import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-18T16:14:28.330310Z",
     "iopub.status.busy": "2024-01-18T16:14:28.330073Z",
     "iopub.status.idle": "2024-01-18T16:14:28.376603Z",
     "shell.execute_reply": "2024-01-18T16:14:28.376043Z",
     "shell.execute_reply.started": "2024-01-18T16:14:28.330286Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be7aad3a3b19409e847d3fa0d427b9b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "UID = 'uid'\n",
    "TIMESTAMP = 'timestamp'\n",
    "MERCATOR_COORD = 'mercator_coord'\n",
    "MERCATOR_X = 'x'\n",
    "MERCATOR_Y = 'y'\n",
    "LATITUDE = 'latitude'\n",
    "LONGITUDE = 'longitude'\n",
    "DATE = 'date'\n",
    "DATE_HOUR = 'date_hour'\n",
    "DAY_OF_WEEK = 'day_of_week'\n",
    "\n",
    "schema = StructType([ \n",
    "    StructField(UID, StringType(), True), \n",
    "    StructField(TIMESTAMP, LongType(), True), \n",
    "    StructField(LATITUDE, DoubleType(), True), \n",
    "    StructField(LONGITUDE, DoubleType(), True), \n",
    "]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-18T15:50:09.141336Z",
     "iopub.status.busy": "2024-01-18T15:50:09.141168Z",
     "iopub.status.idle": "2024-01-18T15:50:09.189669Z",
     "shell.execute_reply": "2024-01-18T15:50:09.189059Z",
     "shell.execute_reply.started": "2024-01-18T15:50:09.141315Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af29440c17b04443be605ded4b547077",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "spark = SparkSession.\\\n",
    "    builder.\\\n",
    "    master(\"local[*]\").\\\n",
    "    appName(\"Example1\").\\\n",
    "    getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intro to DAPHME\n",
    "\n",
    "In this script, we will demonstrate some of DAPHME's functionalities for analyzing human mobility data. Our analysis will focus on the month of February for the city of Philadelphia. Using a dataset that originally contains [60000] users. \n",
    "\n",
    "In our analysis, we will perform three different kinds of operations, which are common in the analyses in these types of data. \n",
    "* **Importing**, which optionally converts the datetime to a specified timezone and project the coordinates into ESPG:3857 (which has units in meters and might be appropriate for a local analysis not too far from the equator). \n",
    "* **Coarse Filtering**, where we subset to users to make computations for tractable. In particular, we filter out users with very few days/hours with activity, and keep TODO [users with \"sufficient\" pings in the area]. We also filter to users with activity in a given time period.\n",
    "* **Persisting**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Pings\n",
    "\n",
    "Daphme can work with a single file in a number of formats (csv, parquet, txt) but its true utility comes when working with partitions of the data, which are often organized in a directory. In this example, we specify the path of the files to be loaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-18T16:42:21.846849Z",
     "iopub.status.busy": "2024-01-18T16:42:21.846587Z",
     "iopub.status.idle": "2024-01-18T16:42:35.139166Z",
     "shell.execute_reply": "2024-01-18T16:42:35.138601Z",
     "shell.execute_reply.started": "2024-01-18T16:42:21.846790Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "30a13ddd7c0d4cef8c290931057fe7aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------+---------+----------+\n",
      "|                 uid|    timestamp| latitude| longitude|\n",
      "+--------------------+-------------+---------+----------+\n",
      "|00000006-23aa-3e4...|1583099437000|39.947813|-74.911673|\n",
      "|00000013-ec62-3ef...|1571218566000|40.013387|-75.144017|\n",
      "|00000013-ec62-3ef...|1571225799000|40.013387|-75.144017|\n",
      "|00000013-ec62-3ef...|1571236576000|40.031884|-75.174015|\n",
      "|00000013-ec62-3ef...|1572230042000|40.013087|-75.094707|\n",
      "|00000013-ec62-3ef...|1572233767000|40.013087|-75.094707|\n",
      "|00000013-ec62-3ef...|1572244569000|40.013087|-75.094707|\n",
      "|00000013-ec62-3ef...|1572248169000|40.013087|-75.094707|\n",
      "|00000013-ec62-3ef...|1572255366000|40.013087|-75.094707|\n",
      "|00000013-ec62-3ef...|1572258968000|40.013087|-75.094707|\n",
      "|00000013-ec62-3ef...|1572262567000|39.980814|-75.172212|\n",
      "|00000013-ec62-3ef...|1572266173000|39.980814|-75.172212|\n",
      "|00000013-ec62-3ef...|1572284181000|40.013087|-75.094707|\n",
      "|00000013-ec62-3ef...|1572287781000|39.980814|-75.172212|\n",
      "|00000013-ec62-3ef...|1572291387000|40.013087|-75.094707|\n",
      "|00000013-ec62-3ef...|1572302166000|40.013087|-75.094707|\n",
      "|00000013-ec62-3ef...|1572309372000|39.980814|-75.172212|\n",
      "|00000013-ec62-3ef...|1573079778000|39.924209|-75.186803|\n",
      "|00000013-ec62-3ef...|1573086964000| 40.05789|-75.082004|\n",
      "|00000013-ec62-3ef...|1575786965000| 39.96382|-75.198219|\n",
      "+--------------------+-------------+---------+----------+\n",
      "only showing top 20 rows"
     ]
    }
   ],
   "source": [
    "# Define schema for the dataset\n",
    "\n",
    "gravy_schema = StructType([ \n",
    "    StructField('grid', StringType()), \n",
    "    StructField('geohashlatitude', StringType()), \n",
    "    StructField('geohashlongitude', StringType()),\n",
    "    StructField('geohashnine', StringType()),\n",
    "    StructField('timestamp', LongType()), \n",
    "    StructField('timezone', StringType()), \n",
    "    StructField('ipaddress', StringType()),\n",
    "    StructField('forensicflag', LongType()), \n",
    "    StructField('devicetype', StringType()),\n",
    "    StructField('recordcount', IntegerType())\n",
    "]) \n",
    "\n",
    "gravy_raw = spark.read.schema(gravy_schema).parquet(\"s3://phl-pings/gravy/\")\n",
    "\n",
    "gravy_raw = gravy_raw. \\\n",
    "    withColumn(\"geohashlatitude\", F.col(\"geohashlatitude\").cast(DoubleType())). \\\n",
    "    withColumn(\"geohashlongitude\", F.col(\"geohashlongitude\").cast(DoubleType()))\n",
    "\n",
    "# We require expected column names for relevant columns\n",
    "\n",
    "gravy_raw = gravy_raw.selectExpr(\"grid as uid\",\n",
    "                                 \"timestamp\",\n",
    "                                 \"geohashlatitude as latitude\",\n",
    "                                 \"geohashlongitude as longitude\")\n",
    "\n",
    "gravy_raw.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-26T00:41:34.074570Z",
     "iopub.status.busy": "2023-12-26T00:41:34.074404Z",
     "iopub.status.idle": "2023-12-26T00:41:49.417609Z",
     "shell.execute_reply": "2023-12-26T00:41:49.416781Z",
     "shell.execute_reply.started": "2023-12-26T00:41:34.074550Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b9906bd98e374039bc9379cc3cc4049e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18440128718"
     ]
    }
   ],
   "source": [
    "gravy_raw.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-26T00:41:49.418761Z",
     "iopub.status.busy": "2023-12-26T00:41:49.418596Z",
     "iopub.status.idle": "2023-12-26T00:41:49.477422Z",
     "shell.execute_reply": "2023-12-26T00:41:49.476576Z",
     "shell.execute_reply.started": "2023-12-26T00:41:49.418740Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "97a0ed9510da458bad844bf0c3d4e536",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#cuebiq_df = spark.read.options(header='True', inferSchema='True', delimiter=',').parquet(\"s3://phl-pings/cuebiq-jan-mar/\")\n",
    "#cuebiq_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert UTC to local datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-18T15:57:19.516201Z",
     "iopub.status.busy": "2024-01-18T15:57:19.515953Z",
     "iopub.status.idle": "2024-01-18T15:57:19.767037Z",
     "shell.execute_reply": "2024-01-18T15:57:19.766412Z",
     "shell.execute_reply.started": "2024-01-18T15:57:19.516167Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "697e937352ce41778dcb3de1108effb1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "gravy_df = cleaning.to_local_time(gravy_raw, 'America/New_York', epoch_unit='milliseconds')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Coarse filter pings to Philadelphia\n",
    "\n",
    "Takes in a geometry and filters to pings within the given geometry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-18T15:57:21.710419Z",
     "iopub.status.busy": "2024-01-18T15:57:21.710195Z",
     "iopub.status.idle": "2024-01-18T15:57:22.541804Z",
     "shell.execute_reply": "2024-01-18T15:57:22.541105Z",
     "shell.execute_reply.started": "2024-01-18T15:57:21.710395Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3bdb9640fa524816a32cb9db590d16dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/hadoop/epidemics-venv/lib64/python3.7/site-packages/boto3/compat.py:82: PythonDeprecationWarning: Boto3 will no longer support Python 3.7 starting December 13, 2023. To continue receiving service updates, bug fixes, and security updates please upgrade to Python 3.8 or later. More information can be found here: https://aws.amazon.com/blogs/developer/python-support-policy-updates-for-aws-sdks-and-tools/\n",
      "  warnings.warn(warning, PythonDeprecationWarning)"
     ]
    }
   ],
   "source": [
    "# Load in a bounding box for Philadelphia\n",
    "\n",
    "import boto3\n",
    "from io import BytesIO\n",
    "\n",
    "s3 = boto3.client('s3')\n",
    "bucket_name = 'upenn-seas-wattscovid19lab'\n",
    "object_key = 'paco/geometry/Census_Tracts_2010.geojson'\n",
    "\n",
    "obj = s3.get_object(Bucket=bucket_name, Key=object_key)\n",
    "geojson_data = json.load(BytesIO(obj['Body'].read()))\n",
    "\n",
    "features = geojson_data['features']\n",
    "\n",
    "polygons = [shape(feature[\"geometry\"]).buffer(0) for feature in features if feature[\"properties\"][\"GEOID10\"][:5] == \"42101\"]\n",
    "phila_poly = unary_union(polygons).buffer(0.0015).simplify(0.0015)\n",
    "phila_box = box(*phila_poly.bounds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-18T15:57:25.037433Z",
     "iopub.status.busy": "2024-01-18T15:57:25.037212Z",
     "iopub.status.idle": "2024-01-18T15:57:26.304302Z",
     "shell.execute_reply": "2024-01-18T15:57:26.303728Z",
     "shell.execute_reply.started": "2024-01-18T15:57:25.037410Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7941416a358045f38c4561d5e528e7af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "phila_box_wkt = phila_box.wkt\n",
    "\n",
    "gravy_df_filtered = cleaning.coarse_filter(gravy_df, phila_box_wkt, spark, id_col=\"grid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert coordinates to mercator in meters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-18T15:57:28.016322Z",
     "iopub.status.busy": "2024-01-18T15:57:28.016082Z",
     "iopub.status.idle": "2024-01-18T15:57:28.267492Z",
     "shell.execute_reply": "2024-01-18T15:57:28.266974Z",
     "shell.execute_reply.started": "2024-01-18T15:57:28.016297Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9b5cf3087e548f49ac75eaf9b339103",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "gravy_df_filtered = cleaning.to_mercator(gravy_df_filtered, spark)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Persist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-26T00:41:51.520971Z",
     "iopub.status.busy": "2023-12-26T00:41:51.520801Z",
     "iopub.status.idle": "2023-12-26T00:57:15.724179Z",
     "shell.execute_reply": "2023-12-26T00:57:15.723616Z",
     "shell.execute_reply.started": "2023-12-26T00:41:51.520949Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08c7a5fb5101444fb3e3ed1955e0c1ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error was encountered:\n",
      "An error occurred while calling o158.parquet.\n",
      ": org.apache.spark.SparkException: Job aborted due to stage failure: Authorized committer (attemptNumber=0, stage=8, partition=282) failed; but task commit success, data duplication may happen. reason=ExecutorLostFailure(13,false,Some(Container marked as failed: container_1703550315317_0002_01_000014 on host: ip-10-0-1-136.us-east-2.compute.internal. Exit status: -100. Diagnostics: Container released on a *lost* node.))\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2974)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2910)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2909)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2909)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleStageFailed$1(DAGScheduler.scala:1256)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleStageFailed$1$adapted(DAGScheduler.scala:1256)\n",
      "\tat scala.Option.foreach(Option.scala:407)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleStageFailed(DAGScheduler.scala:1256)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3170)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3112)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3101)\n",
      "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:1028)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2267)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$executeWrite$4(FileFormatWriter.scala:307)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.writeAndCommit(FileFormatWriter.scala:271)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeWrite(FileFormatWriter.scala:304)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:190)\n",
      "\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:197)\n",
      "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)\n",
      "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)\n",
      "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)\n",
      "\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$executeCollect$1(AdaptiveSparkPlanExec.scala:516)\n",
      "\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.withFinalPlanUpdate(AdaptiveSparkPlanExec.scala:555)\n",
      "\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.executeCollect(AdaptiveSparkPlanExec.scala:516)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:104)\n",
      "\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:107)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withTracker(SQLExecution.scala:250)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.executeQuery$1(SQLExecution.scala:123)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$9(SQLExecution.scala:160)\n",
      "\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:107)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withTracker(SQLExecution.scala:250)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$8(SQLExecution.scala:160)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:271)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:159)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:69)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:101)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:97)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:554)\n",
      "\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:107)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:554)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:530)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:97)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:84)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:82)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:142)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:856)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:387)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:360)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:239)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.parquet(DataFrameWriter.scala:789)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/mnt1/yarn/usercache/livy/appcache/application_1703550315317_0002/container_1703550315317_0002_01_000001/pyspark.zip/pyspark/sql/readwriter.py\", line 1656, in parquet\n",
      "    self._jwrite.parquet(path)\n",
      "  File \"/mnt1/yarn/usercache/livy/appcache/application_1703550315317_0002/container_1703550315317_0002_01_000001/py4j-0.10.9.7-src.zip/py4j/java_gateway.py\", line 1323, in __call__\n",
      "    answer, self.gateway_client, self.target_id, self.name)\n",
      "  File \"/mnt1/yarn/usercache/livy/appcache/application_1703550315317_0002/container_1703550315317_0002_01_000001/pyspark.zip/pyspark/errors/exceptions/captured.py\", line 169, in deco\n",
      "    return f(*a, **kw)\n",
      "  File \"/mnt1/yarn/usercache/livy/appcache/application_1703550315317_0002/container_1703550315317_0002_01_000001/py4j-0.10.9.7-src.zip/py4j/protocol.py\", line 328, in get_return_value\n",
      "    format(target_id, \".\", name), value)\n",
      "py4j.protocol.Py4JJavaError: An error occurred while calling o158.parquet.\n",
      ": org.apache.spark.SparkException: Job aborted due to stage failure: Authorized committer (attemptNumber=0, stage=8, partition=282) failed; but task commit success, data duplication may happen. reason=ExecutorLostFailure(13,false,Some(Container marked as failed: container_1703550315317_0002_01_000014 on host: ip-10-0-1-136.us-east-2.compute.internal. Exit status: -100. Diagnostics: Container released on a *lost* node.))\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2974)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2910)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2909)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2909)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleStageFailed$1(DAGScheduler.scala:1256)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleStageFailed$1$adapted(DAGScheduler.scala:1256)\n",
      "\tat scala.Option.foreach(Option.scala:407)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleStageFailed(DAGScheduler.scala:1256)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3170)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3112)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3101)\n",
      "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:1028)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2267)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$executeWrite$4(FileFormatWriter.scala:307)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.writeAndCommit(FileFormatWriter.scala:271)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeWrite(FileFormatWriter.scala:304)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:190)\n",
      "\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:197)\n",
      "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)\n",
      "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)\n",
      "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)\n",
      "\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$executeCollect$1(AdaptiveSparkPlanExec.scala:516)\n",
      "\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.withFinalPlanUpdate(AdaptiveSparkPlanExec.scala:555)\n",
      "\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.executeCollect(AdaptiveSparkPlanExec.scala:516)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:104)\n",
      "\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:107)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withTracker(SQLExecution.scala:250)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.executeQuery$1(SQLExecution.scala:123)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$9(SQLExecution.scala:160)\n",
      "\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:107)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withTracker(SQLExecution.scala:250)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$8(SQLExecution.scala:160)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:271)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:159)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:69)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:101)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:97)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:554)\n",
      "\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:107)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:554)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:530)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:97)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:84)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:82)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:142)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:856)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:387)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:360)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:239)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.parquet(DataFrameWriter.scala:789)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "gravy_df_clean = gravy_df_filtered.select(\n",
    "    'grid', \n",
    "    'local_timestamp', \n",
    "    'mercator_coord', \n",
    "    'x',\n",
    "    'y',\n",
    "    'in_geo',\n",
    "    'date', \n",
    "    'date_hour', \n",
    "    'day_of_week'\n",
    ").withColumnRenamed(\"grid\", \"identifier\")\n",
    "\n",
    "gravy_df_clean.write.partitionBy(\"date\").option(\"header\", \"true\").mode(\"overwrite\").parquet(\"s3://phl-pings/gravy_clean/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-26T01:05:55.545163Z",
     "iopub.status.busy": "2023-12-26T01:05:55.544919Z",
     "iopub.status.idle": "2023-12-26T01:14:37.458298Z",
     "shell.execute_reply": "2023-12-26T01:14:37.457745Z",
     "shell.execute_reply.started": "2023-12-26T01:05:55.545138Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c9623a48d0946f8a36b8a1c76ad3f50",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+--------------------+-------------------+------------------+------+----------+-------------+-----------+\n",
      "|          identifier|    local_timestamp|      mercator_coord|                  x|                 y|in_geo|      date|    date_hour|day_of_week|\n",
      "+--------------------+-------------------+--------------------+-------------------+------------------+------+----------+-------------+-----------+\n",
      "|57f80ddf-e272-37c...|2020-01-05 06:28:39|POINT (-8339129.2...| -8339129.292832218| 4858361.502393234| false|2020-01-05|2020-01-05 06|          1|\n",
      "|57f80ddf-e272-37c...|2020-01-07 18:43:31|POINT (-8367196.0...| -8367196.053406948| 4859015.823786038|  true|2020-01-07|2020-01-07 18|          3|\n",
      "|57f904d1-7a14-436...|2020-03-23 03:30:28|POINT (-8369513.0...| -8369513.057288318| 4854598.321212323|  true|2020-03-23|2020-03-23 03|          2|\n",
      "|57fb1ca6-13fe-39d...|2020-04-25 12:26:00|POINT (-8362108.1...|  -8362108.19608024|  4865112.55255486|  true|2020-04-25|2020-04-25 12|          7|\n",
      "|57fb1ca6-13fe-39d...|2020-04-26 14:15:35|POINT (-8362108.1...|  -8362108.19608024|  4865112.55255486|  true|2020-04-26|2020-04-26 14|          1|\n",
      "|57fb1ca6-13fe-39d...|2020-04-26 14:32:21|POINT (-8362108.1...|  -8362108.19608024|  4865112.55255486|  true|2020-04-26|2020-04-26 14|          1|\n",
      "|57fb1ca6-13fe-39d...|2020-04-26 14:38:18|POINT (-8362108.1...|  -8362108.19608024|  4865112.55255486|  true|2020-04-26|2020-04-26 14|          1|\n",
      "|57fb1ca6-13fe-39d...|2020-04-26 14:46:59|POINT (-8362108.1...|  -8362108.19608024|  4865112.55255486|  true|2020-04-26|2020-04-26 14|          1|\n",
      "|57fb1ca6-13fe-39d...|2020-04-26 14:48:58|POINT (-8362108.1...|  -8362108.19608024|  4865112.55255486|  true|2020-04-26|2020-04-26 14|          1|\n",
      "|57fbbf62-22f5-325...|2019-10-14 05:09:03|POINT (-8393390.0...|  -8393390.08618806|4854149.8135938365| false|2019-10-14|2019-10-14 05|          2|\n",
      "|57fbbf62-22f5-325...|2019-10-14 05:09:39|POINT (-8388564.9...| -8388564.942859624| 4844107.525975829| false|2019-10-14|2019-10-14 05|          2|\n",
      "|57fbbf62-22f5-325...|2019-10-14 05:15:31|POINT (-8415418.2...|  -8415418.20966521| 4802855.952446476| false|2019-10-14|2019-10-14 05|          2|\n",
      "|57fbbf62-22f5-325...|2019-10-14 08:22:37|POINT (-8367196.0...| -8367196.053406948| 4859015.823786038|  true|2019-10-14|2019-10-14 08|          2|\n",
      "|57fbbf62-22f5-325...|2019-10-14 08:22:50|POINT (-8367196.0...| -8367196.053406948| 4859015.823786038|  true|2019-10-14|2019-10-14 08|          2|\n",
      "|57fc662a-93ec-3fb...|2019-10-14 04:31:59|POINT (-8379153.6...| -8379153.659149488| 4865711.227726414|  true|2019-10-14|2019-10-14 04|          2|\n",
      "|57fc82e9-7fc1-314...|2019-11-07 18:00:18|POINT (-8362251.4...| -8362251.464264891| 4871175.662858475|  true|2019-11-07|2019-11-07 18|          5|\n",
      "|57fc82e9-7fc1-314...|2019-11-07 18:02:52|POINT (-8362242.0...|-8362242.0021081725|4870957.2835251745|  true|2019-11-07|2019-11-07 18|          5|\n",
      "|57fc82e9-7fc1-314...|2019-11-07 18:12:47|POINT (-8362242.0...|-8362242.0021081725|4870957.2835251745|  true|2019-11-07|2019-11-07 18|          5|\n",
      "|57fc82e9-7fc1-314...|2019-11-07 20:26:27|POINT (-8362242.0...|-8362242.0021081725|4870957.2835251745|  true|2019-11-07|2019-11-07 20|          5|\n",
      "|57fc82e9-7fc1-314...|2019-11-07 20:26:45|POINT (-8362251.4...| -8362251.464264891| 4871175.662858475|  true|2019-11-07|2019-11-07 20|          5|\n",
      "+--------------------+-------------------+--------------------+-------------------+------------------+------+----------+-------------+-----------+\n",
      "only showing top 20 rows"
     ]
    }
   ],
   "source": [
    "gravy_df_clean.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Sparkmagic (PySpark)",
   "language": "python",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-25T22:36:36.091505Z",
     "iopub.status.busy": "2023-12-25T22:36:36.091216Z",
     "iopub.status.idle": "2023-12-25T22:36:36.103329Z",
     "shell.execute_reply": "2023-12-25T22:36:36.102849Z",
     "shell.execute_reply.started": "2023-12-25T22:36:36.091480Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "Current session configs: <tt>{'conf': {'spark.pyspark.python': '/home/hadoop/epidemics-venv/bin/python3', 'spark.pyspark.virtualenv.bin.path': '/home/hadoop/epidemics-venv/bin', 'spark.dynamicAllocation.enabled': 'true', 'spark.executor.memory': '16g', 'spark.driver.memory': '16g', 'spark.executor.cores': '6', 'spark.driver.cores': '6', 'spark.driver.maxResultSize': '20g', 'livy.server.session.timeout-check': 'false'}, 'proxyUser': 'user_thomas-li', 'kind': 'pyspark'}</tt><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "No active sessions."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%configure -f\n",
    "\n",
    "{\"conf\":\n",
    "     {\"spark.pyspark.python\":\"/home/hadoop/epidemics-venv/bin/python3\",\n",
    "      \"spark.pyspark.virtualenv.bin.path\":\"/home/hadoop/epidemics-venv/bin\",\n",
    "      \"spark.pyspark.python\":\"/home/hadoop/epidemics-venv/bin/python3\",\n",
    "      \"spark.dynamicAllocation.enabled\":\"true\",\n",
    "      \"spark.executor.memory\":\"16g\",\n",
    "      \"spark.driver.memory\":\"16g\",\n",
    "      \"spark.executor.cores\":\"6\",\n",
    "      \"spark.driver.cores\":\"6\",\n",
    "      \"spark.driver.maxResultSize\":\"20g\",\n",
    "      \"livy.server.session.timeout-check\" :\"false\"}\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-25T22:36:37.734706Z",
     "iopub.status.busy": "2023-12-25T22:36:37.734495Z",
     "iopub.status.idle": "2023-12-25T22:37:04.521938Z",
     "shell.execute_reply": "2023-12-25T22:37:04.521433Z",
     "shell.execute_reply.started": "2023-12-25T22:36:37.734683Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b09608d3b7cb440a97639fbc94471ec6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n<tbody><tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>User</th><th>Current session?</th></tr><tr><td>3</td><td>application_1703539116779_0004</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-10-0-1-97.us-east-2.compute.internal:20888/proxy/application_1703539116779_0004/\" class=\"emr-proxy-link j-1AKGHERV6WKNX application_1703539116779_0004\" emr-resource=\"j-1AKGHERV6WKNX\n\" application-id=\"application_1703539116779_0004\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-10-0-1-96.us-east-2.compute.internal:8042/node/containerlogs/container_1703539116779_0004_01_000001/livy\">Link</a></td><td>None</td><td>✔</td></tr></tbody></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import json\n",
    "import shapely\n",
    "from shapely.geometry import Point, MultiPoint, Polygon, shape, box\n",
    "from shapely.ops import unary_union, transform\n",
    "\n",
    "from pyspark.sql import DataFrame, SparkSession\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import DoubleType, ArrayType\n",
    "\n",
    "import daphme as dm\n",
    "from daphme import cleaning\n",
    "\n",
    "from sedona.spark import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-25T22:37:04.523008Z",
     "iopub.status.busy": "2023-12-25T22:37:04.522855Z",
     "iopub.status.idle": "2023-12-25T22:37:04.561059Z",
     "shell.execute_reply": "2023-12-25T22:37:04.560536Z",
     "shell.execute_reply.started": "2023-12-25T22:37:04.522989Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3aa7e507b97d425385c2ece17f502565",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "spark = SparkSession.\\\n",
    "    builder.\\\n",
    "    master(\"local[*]\").\\\n",
    "    appName(\"Example1\").\\\n",
    "    getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-25T22:37:04.562482Z",
     "iopub.status.busy": "2023-12-25T22:37:04.562327Z",
     "iopub.status.idle": "2023-12-25T22:37:04.601094Z",
     "shell.execute_reply": "2023-12-25T22:37:04.600612Z",
     "shell.execute_reply.started": "2023-12-25T22:37:04.562463Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3241fdfa26d841249ecd7277eec988bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.sql import DataFrame, SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from sedona.spark import *\n",
    "\n",
    "def to_local_time(df: DataFrame, \n",
    "                  timezone_to: str,\n",
    "                  timestamp_col: str = \"timestamp\",\n",
    "                  epoch_unit: str = \"seconds\") -> DataFrame:\n",
    "    \"\"\"Transforms a column of epoch times in a Spark DataFrame to a local time zone specified timezone_to. Additional columns for date, hour, and day of the week are also added to the DataFrame.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : DataFrame\n",
    "        A Spark DataFrame containing a column with epoch times to be converted.\n",
    "\n",
    "    timezone_to : str\n",
    "        A valid timezone identifier for the local timezone of the data (e.g., \"America/New_York\", \"UTC\").\n",
    "\n",
    "    timestamp_col : str (default \"timestamp\")\n",
    "        The name of the column in 'df' containing epoch times.\n",
    "\n",
    "    epoch_unit : str (default \"seconds\")\n",
    "        The unit of epoch time in the 'timestamp_col'. Acceptable values include \"seconds\", \"milliseconds\", \n",
    "        \"microseconds\", and \"nanoseconds\". Defaults to \"seconds\" if not specified.\n",
    "\n",
    "    Returns\n",
    "    ----------\n",
    "    DataFrame\n",
    "        A new Spark DataFrame with all original columns from 'df' and the following additional columns:\n",
    "            - 'local_timestamp': The local timestamp derived from the original epoch time.\n",
    "            - 'date': The date extracted from 'local_timestamp'.\n",
    "            - 'date_hour': The date and hour extracted from 'local_timestamp'.\n",
    "            - 'day_of_week': The day of the week (1 for Sunday, 2 for Monday, ..., 7 for Saturday) derived from 'local_timestamp'.\n",
    "\n",
    "    Example\n",
    "    ----------\n",
    "    >>> # Assuming a SparkSession `spark` and a DataFrame `df` with a 'timestamp' column are predefined.\n",
    "    >>> timezone_str = \"America/New_York\"\n",
    "    >>> converted_df = to_local_time(df, timezone_str)\n",
    "    >>> converted_df.show()\n",
    "    \"\"\"\n",
    "\n",
    "    divisor = {\n",
    "        \"seconds\": 1,\n",
    "        \"milliseconds\": 1000,\n",
    "        \"microseconds\": 1000000,\n",
    "        \"nanoseconds\": 1000000000\n",
    "    }.get(epoch_unit, 1)\n",
    "\n",
    "    df = df.withColumn(\n",
    "        \"local_timestamp\",\n",
    "        F.from_utc_timestamp(\n",
    "            F.to_timestamp(F.col(timestamp_col) / divisor),\n",
    "            timezone_to\n",
    "        ))\n",
    "\n",
    "    df = df.withColumn(\n",
    "        \"date\",\n",
    "        F.to_date(F.col(\"local_timestamp\"))\n",
    "    ).withColumn(\n",
    "        \"date_hour\",\n",
    "        F.date_format(F.col(\"local_timestamp\"), \"yyyy-MM-dd HH\")\n",
    "    ).withColumn(\n",
    "        \"day_of_week\",\n",
    "        F.dayofweek(F.col(\"local_timestamp\"))\n",
    "    )\n",
    "\n",
    "    return df\n",
    "\n",
    "def to_mercator(df: DataFrame, \n",
    "                spark: SparkSession,\n",
    "                longitude_col: str = \"longitude\", \n",
    "                latitude_col: str = \"latitude\") -> DataFrame:\n",
    "    \"\"\"Converts geographic coordinates from EPSG:4326 to EPSG:3857 (Web Mercator projection) and appends the result as a new column 'mercator_coord' to the input DataFrame.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df : DataFrame\n",
    "        A Spark DataFrame containing columns corresponding to longitude and latitude values in EPSG:4326.\n",
    "\n",
    "    spark : SparkSession\n",
    "        The active SparkSession instance used to execute Spark SQL operations.\n",
    "\n",
    "    Returns\n",
    "    ----------\n",
    "    DataFrame\n",
    "        A new Spark DataFrame with all original columns from 'df' and an additional column 'mercator_coord' containing the geometry (point) of the original latitude and longitude values transformed to the EPSG:3857 coordinate system.\n",
    "\n",
    "    Example\n",
    "    ----------\n",
    "    >>> # Assuming a SparkSession `spark` and a DataFrame `df` are predefined\n",
    "    >>> mercator_df = to_mercator(df, spark)\n",
    "    >>> mercator_df.show()\n",
    "    \"\"\"\n",
    "    \n",
    "    df.createOrReplaceTempView(\"df\")\n",
    "    \n",
    "    query = f\"\"\"\n",
    "        SELECT *,\n",
    "               ST_FlipCoordinates(\n",
    "                   ST_Transform(\n",
    "                       ST_MakePoint({longitude_col}, {latitude_col}), \n",
    "                       'EPSG:4326', 'EPSG:3857'\n",
    "                   )\n",
    "               ) AS mercator_coord\n",
    "        FROM df\n",
    "        \"\"\"\n",
    "    \n",
    "    return spark.sql(query)\n",
    "\n",
    "def coarse_filter(df: DataFrame, \n",
    "                  bounding_wkt: str, \n",
    "                  spark: SparkSession,\n",
    "                  longitude_col: str = \"longitude\", \n",
    "                  latitude_col: str = \"latitude\", \n",
    "                  id_col: str = \"id\") -> DataFrame:\n",
    "    \"\"\"Filters a DataFrame based on whether geographical points (defined by longitude and latitude) fall within a specified geometry.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : DataFrame\n",
    "        The Spark DataFrame to be filtered. It should contain columns corresponding to longitude and latitude values, as well as an id column.\n",
    "    \n",
    "    bounding_wkt : str\n",
    "        The Well-Known Text (WKT) string representing the bounding geometry within which points are tested for inclusion. The WKT should define a polygon in the EPSG:4326 coordinate reference system.\n",
    "    \n",
    "    spark : SparkSession\n",
    "        The active SparkSession instance used to execute Spark operations.\n",
    "    \n",
    "    longitude_col : str, default \"longitude\"\n",
    "        The name of the column in 'df' containing longitude values. Longitude values should be in the EPSG:4326 coordinate reference system.\n",
    "    \n",
    "    latitude_col : str, default \"latitude\"\n",
    "        The name of the column in 'df' containing latitude values. Latitude values should be in the EPSG:4326 coordinate reference system.\n",
    "    \n",
    "    id_col : str, default \"id\"\n",
    "        The name of the column in 'df' containing user IDs.\n",
    "    \n",
    "    Returns\n",
    "    ----------\n",
    "    DataFrame\n",
    "        A new Spark DataFrame filtered to include only rows where the point (longitude, latitude) falls within the specified geometric boundary defined by 'bounding_wkt'. This DataFrame includes all original columns from 'df'.\n",
    "\n",
    "    Example\n",
    "    ----------\n",
    "    >>> # Assuming a SparkSession `spark` and a DataFrame `df` are predefined\n",
    "    >>> bounding_wkt = \"POLYGON((...))\"  # Replace with actual WKT\n",
    "    >>> filtered_df = coarse_filter(df, bounding_wkt, spark)\n",
    "    >>> filtered_df.show()\n",
    "    \"\"\"\n",
    "    \n",
    "    df = df.withColumn(\"coordinate\", F.expr(f\"ST_MakePoint({longitude_col}, {latitude_col})\"))\n",
    "    df.createOrReplaceTempView(\"temp_df\")\n",
    "    \n",
    "    query = f\"\"\"\n",
    "        WITH UniqueIDs AS (\n",
    "            SELECT DISTINCT {id_col} AS id\n",
    "            FROM temp_df\n",
    "            WHERE ST_Contains(ST_GeomFromWKT('{bounding_wkt}'), coordinate)\n",
    "        )\n",
    "\n",
    "        SELECT t.*\n",
    "        FROM temp_df t\n",
    "        INNER JOIN UniqueIDs u ON t.{id_col} = u.id\n",
    "        \"\"\"\n",
    "    \n",
    "    return spark.sql(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intro to DAPHME\n",
    "\n",
    "In this script, we will demonstrate some of DAPHME's functionalities for analyzing human mobility data. Our analysis will focus on the month of February for the city of Philadelphia. Using a dataset that originally contains [60000] users. \n",
    "\n",
    "In our analysis, we will perform three different kinds of operations, which are common in the analyses in these types of data. \n",
    "* **Importing**, which optionally converts the datetime to a specified timezone and project the coordinates into ESPG:3857 (which has units in meters and might be appropriate for a local analysis not too far from the equator). \n",
    "* **Coarse Filtering**, where we subset to users to make computations for tractable. In particular, we filter out users with very few days/hours with activity, and keep TODO [users with \"sufficient\" pings in the area]. We also filter to users with activity in a given time period.\n",
    "* **Persisting**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Pings\n",
    "\n",
    "Daphme can work with a single file in a number of formats (csv, parquet, txt) but its true utility comes when working with partitions of the data, which are often organized in a directory. In this example, we specify the path of the files to be loaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-25T22:38:48.830488Z",
     "iopub.status.busy": "2023-12-25T22:38:48.830259Z",
     "iopub.status.idle": "2023-12-25T22:38:50.080344Z",
     "shell.execute_reply": "2023-12-25T22:38:50.079839Z",
     "shell.execute_reply.started": "2023-12-25T22:38:48.830464Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3875d360497f4a7e97a3c6e3dbd5b245",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#gravy_raw = spark.read.parquet(\"s3://phl-pings/gravy/\")\n",
    "gravy_raw = spark.read.parquet(\"s3://phl-pings/gravy/part_0.snappy.parquet\")\n",
    "\n",
    "gravy_raw = gravy_raw. \\\n",
    "    withColumn(\"latitude\", F.col(\"geohashlatitude\").cast(DoubleType())). \\\n",
    "    withColumn(\"longitude\", F.col(\"geohashlongitude\").cast(DoubleType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-25T22:37:11.878118Z",
     "iopub.status.busy": "2023-12-25T22:37:11.877963Z",
     "iopub.status.idle": "2023-12-25T22:37:19.150290Z",
     "shell.execute_reply": "2023-12-25T22:37:19.149790Z",
     "shell.execute_reply.started": "2023-12-25T22:37:11.878099Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eded728ecd5d4069b99a472b72778b02",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- grid: string (nullable = true)\n",
      " |-- geohashlatitude: string (nullable = true)\n",
      " |-- geohashlongitude: string (nullable = true)\n",
      " |-- geohashnine: string (nullable = true)\n",
      " |-- timestamp: long (nullable = true)\n",
      " |-- timezone: string (nullable = true)\n",
      " |-- ipaddress: string (nullable = true)\n",
      " |-- forensicflag: long (nullable = true)\n",
      " |-- devicetype: string (nullable = true)\n",
      " |-- recordcount: integer (nullable = true)\n",
      " |-- latitude: double (nullable = true)\n",
      " |-- longitude: double (nullable = true)"
     ]
    }
   ],
   "source": [
    "gravy_raw.count()\n",
    "gravy_raw.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#cuebiq_df = spark.read.options(header='True', inferSchema='True', delimiter=',').parquet(\"s3://phl-pings/cuebiq-jan-mar/\")\n",
    "#cuebiq_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert UTC to local datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-25T22:41:15.829115Z",
     "iopub.status.busy": "2023-12-25T22:41:15.828896Z",
     "iopub.status.idle": "2023-12-25T22:41:16.072451Z",
     "shell.execute_reply": "2023-12-25T22:41:16.071960Z",
     "shell.execute_reply.started": "2023-12-25T22:41:15.829092Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad165a07191f4d3dbf08610c85e207f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "gravy_df = cleaning.to_local_time(gravy_raw, 'America/New_York', epoch_unit='milliseconds')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Coarse filter pings to Philadelphia\n",
    "\n",
    "Takes in a geometry and filters to pings within the given geometry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-25T22:37:19.393842Z",
     "iopub.status.busy": "2023-12-25T22:37:19.393687Z",
     "iopub.status.idle": "2023-12-25T22:37:20.137127Z",
     "shell.execute_reply": "2023-12-25T22:37:20.136621Z",
     "shell.execute_reply.started": "2023-12-25T22:37:19.393823Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "11db602b9d7f4b48ae0d58225e0670a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/hadoop/epidemics-venv/lib64/python3.7/site-packages/boto3/compat.py:82: PythonDeprecationWarning: Boto3 will no longer support Python 3.7 starting December 13, 2023. To continue receiving service updates, bug fixes, and security updates please upgrade to Python 3.8 or later. More information can be found here: https://aws.amazon.com/blogs/developer/python-support-policy-updates-for-aws-sdks-and-tools/\n",
      "  warnings.warn(warning, PythonDeprecationWarning)"
     ]
    }
   ],
   "source": [
    "# Load in a bounding box for Philadelphia\n",
    "\n",
    "import boto3\n",
    "from io import BytesIO\n",
    "\n",
    "s3 = boto3.client('s3')\n",
    "bucket_name = 'upenn-seas-wattscovid19lab'\n",
    "object_key = 'paco/geometry/Census_Tracts_2010.geojson'\n",
    "\n",
    "obj = s3.get_object(Bucket=bucket_name, Key=object_key)\n",
    "geojson_data = json.load(BytesIO(obj['Body'].read()))\n",
    "\n",
    "features = geojson_data['features']\n",
    "\n",
    "polygons = [shape(feature[\"geometry\"]).buffer(0) for feature in features if feature[\"properties\"][\"GEOID10\"][:5] == \"42101\"]\n",
    "phila_poly = unary_union(polygons).buffer(0.0015).simplify(0.0015)\n",
    "phila_box = box(*phila_poly.bounds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-25T22:41:26.833778Z",
     "iopub.status.busy": "2023-12-25T22:41:26.833566Z",
     "iopub.status.idle": "2023-12-25T22:41:27.083245Z",
     "shell.execute_reply": "2023-12-25T22:41:27.082619Z",
     "shell.execute_reply.started": "2023-12-25T22:41:26.833756Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf809b7f22484f07938116f69a783892",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "phila_box_wkt = phila_box.wkt\n",
    "\n",
    "gravy_df_filtered = cleaning.coarse_filter(gravy_df, phila_box_wkt, spark, id_col=\"grid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert coordinates to mercator in meters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-25T22:41:35.294829Z",
     "iopub.status.busy": "2023-12-25T22:41:35.294595Z",
     "iopub.status.idle": "2023-12-25T22:41:35.542247Z",
     "shell.execute_reply": "2023-12-25T22:41:35.541603Z",
     "shell.execute_reply.started": "2023-12-25T22:41:35.294796Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb4ce0ed12f44985841f7499ac1c5839",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "gravy_df_filtered = cleaning.to_mercator(gravy_df_filtered, spark)\n",
    "gravy_df_filtered = gravy_df_filtered.select(\"grid\", \"local_timestamp\", \"mercator_coord\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-25T22:41:44.569747Z",
     "iopub.status.busy": "2023-12-25T22:41:44.569533Z",
     "iopub.status.idle": "2023-12-25T22:42:11.901646Z",
     "shell.execute_reply": "2023-12-25T22:42:11.901138Z",
     "shell.execute_reply.started": "2023-12-25T22:41:44.569724Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "59da45ae9910415385f832e278e0ac75",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+--------------------+\n",
      "|                grid|    local_timestamp|      mercator_coord|\n",
      "+--------------------+-------------------+--------------------+\n",
      "|00002b4c-410c-3a1...|2020-03-31 19:25:58|POINT (4850108.06...|\n",
      "|0004887b-01de-392...|2019-11-27 17:24:09|POINT (4859015.82...|\n",
      "|0004d938-bcf3-380...|2019-10-10 16:23:09|POINT (4956193.59...|\n",
      "|0004d938-bcf3-380...|2019-10-10 16:23:51|POINT (4956193.59...|\n",
      "|0004d938-bcf3-380...|2019-10-10 16:23:51|POINT (4956193.59...|\n",
      "|0004d938-bcf3-380...|2019-10-10 16:24:05|POINT (4956199.90...|\n",
      "|0004d938-bcf3-380...|2019-10-10 16:24:05|POINT (4956199.90...|\n",
      "|0004d938-bcf3-380...|2019-10-10 16:24:09|POINT (4956199.90...|\n",
      "|0004d938-bcf3-380...|2019-10-10 16:24:21|POINT (4956199.90...|\n",
      "|0004d938-bcf3-380...|2019-10-10 16:24:33|POINT (4956199.90...|\n",
      "|0004d938-bcf3-380...|2019-10-10 16:24:33|POINT (4956199.90...|\n",
      "|0004d938-bcf3-380...|2019-10-10 16:24:55|POINT (4956199.90...|\n",
      "|0004d938-bcf3-380...|2019-10-10 16:24:55|POINT (4956199.90...|\n",
      "|0004d938-bcf3-380...|2019-10-10 16:25:07|POINT (4956199.90...|\n",
      "|0004d938-bcf3-380...|2019-10-10 16:25:08|POINT (4956199.90...|\n",
      "|0004d938-bcf3-380...|2019-10-10 16:25:09|POINT (4956199.90...|\n",
      "|0004d938-bcf3-380...|2019-10-10 16:25:14|POINT (4956199.90...|\n",
      "|0004d938-bcf3-380...|2019-10-10 16:25:19|POINT (4956199.90...|\n",
      "|0004d938-bcf3-380...|2019-10-10 16:25:44|POINT (4956199.90...|\n",
      "|0004d938-bcf3-380...|2019-10-10 16:25:44|POINT (4956199.90...|\n",
      "+--------------------+-------------------+--------------------+\n",
      "only showing top 20 rows"
     ]
    }
   ],
   "source": [
    "gravy_df_filtered.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-25T22:39:47.338282Z",
     "iopub.status.busy": "2023-12-25T22:39:47.338068Z",
     "iopub.status.idle": "2023-12-25T22:39:58.625766Z",
     "shell.execute_reply": "2023-12-25T22:39:58.625053Z",
     "shell.execute_reply.started": "2023-12-25T22:39:47.338259Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6b5bdff1cc44910ad97f7ce63388f0a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30005324"
     ]
    }
   ],
   "source": [
    "gravy_df_filtered.count()\n",
    "#gravy_df_filtered.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Persist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = gravy_df_filtered.select(\n",
    "    'grid', \n",
    "    'local_timestamp', \n",
    "    'mercator_coord', \n",
    "    'date', \n",
    "    'date_hour', \n",
    "    'day_of_week')\n",
    "output.write.partitionBy(\"date\").option(\"header\", \"true\").mode(\"overwrite\").parquet(\"s3://phl-pings/gravy_clean/\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
